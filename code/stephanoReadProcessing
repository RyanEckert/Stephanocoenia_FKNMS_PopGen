################################################################################
################################################################################
#
#
#
#
################################################################################
#################################  S E T U P  ##################################
# Download all necessary scripts and load modules for processing/analysis      #
#------------------------------------------------------------------------------#
#--login to KoKo
ssh reckert2017@koko-login.hpc.fau.edu

#--load necessary modules (or add to ~/.bashrc to load at login)
module load angsd-0.933-gcc-9.2.0-65d64pp
module load bowtie2-2.3.5.1-gcc-8.3.0-63cvhw5
module load cdhit-4.8.1-gcc-8.3.0-bcay75d
module load kraken2-2.1.1-gcc-9.2.0-ocivj3u
module load launcher
module load miniconda3-4.6.14-gcc-8.3.0-eenl5dj
module load ncbi-toolkit-22_0_0-gcc-9.2.0-jjhd2wa
module load ngsadmix-32-gcc-8.3.0-qbnwmpq
module load python-3.7.4-gcc-8.3.0-3tniqr5
module load R/3.6.1
module load samtools-1.10-gcc-8.3.0-khgksad

#--put scripts needed into ~/bin or similar directory that is mapped in .bashrc path
cd ~/bin
svn checkout https://github.com/ryanjeckert/fknmsStephanocoenia/trunk/scripts/
mv scripts/* .
rm scripts

#--make all scripts executable
chmod +x *.sh *.pl *.py

#--build working directory
mkdir 2bRAD/sint/
mkdir 2bRAD/sint/fknms/
mkdir 2bRAD/sint/fknms/rawReads/
cd 2bRAD/sint/fknms/rawReads/


################################################################################
#########################  D O W N L O A D  R E A D S  #########################
# Download and concatenate raw reads from BaseSpace                            #
#------------------------------------------------------------------------------#
#--making a script to download the reads and merge samples across 2 NovaSeq lanes
echo '#!/bin/bash' > downloadReads.sh
echo 'bs download project --concurrency=high -q -n JA21001 -o .' >> downloadReads.sh
# -n is the project name and -o is the output directory

echo "find . -name '*.gz' -exec mv {} . \;" >> downloadReads.sh
echo 'rmdir SA*' >>downloadReads.sh
echo 'mkdir ../concatReads' >> downloadReads.sh
echo 'cp *.gz ../concatReads' >> downloadReads.sh
echo 'cd ../concatReads' >> downloadReads.sh
echo 'mergeReads.sh -o mergeTemp' >> downloadReads.sh
# -o is the directory to put output files in

echo 'rm *L00*' >> downloadReads.sh
echo "find . -name '*.gz' -exec mv {} . \;" >> downloadReads.sh
echo 'gunzip *.gz' >> downloadReads.sh
echo 'rmdir mergeTemp' >> downloadReads.sh

chmod +x downloadReads.sh

launcher_creator.py -b 'srun downloadReads.sh' -n downloadReads -q shortq7 -t 06:00:00 -e reckert2017@fau.edu
sbatch downloadReads.slurm

################################################################################
###################  T R I M M I N G  &  F I L T E R I N G  ####################
# Trim and demultiplex reads                                                   #
#------------------------------------------------------------------------------#
cd ../concatReads

2bRAD_trim_launch_dedup.pl fastq > trims.sh
launcher_creator.py -j trims.sh -n trims -q shortq7 -t 06:00:00 -e reckert2017@fau.edu
sbatch --mem=200GB trims.slurm


# check that we have the correct number of trim files (226 in this case)
ls -l *.tr0 | wc -l

mkdir ../trimmedReads
srun mv *.tr0 ../trimmedReads &

ls *.fastq | cat | sed -e 's/^/gzip -9 /' > gzip.sh
launcher_creator.py -j gzip.sh -n gzip -q shortq7 -t 06:00:00 -e reckert2017@fau.edu
sbatch gzip.slurm

cd ../trimmedReads

# rename sequence files using sampleRename.py
# make sure you use the reverse complement of your inline BCs!
srun sampleRename.py -i sampleList -n fk_ -f tr0


## Quality filtering using cutadapt
#-------------------------------------------------------------------------------
# I can't get KoKo's module for cutadapt to work, so we'll do it in miniconda

##--create conda environment
##--uncomment and run below if you don't have a conda env. set up
#module load miniconda3-4.6.14-gcc-8.3.0-eenl5dj
#conda config --add channels defaults
#conda config --add channels bioconda
#conda config --add channels conda-forge
#conda create -n 2bRAD cutadapt

source activate 2bRAD

# Removing reads with qualities at ends less than Q15 for de novo analysis
echo '#!/bin/bash' > trimse.sh
echo 'module load miniconda3-4.6.14-gcc-8.3.0-eenl5dj' >> trimse.sh
echo 'source activate 2bRAD' >> trimse.sh
for file in *.tr0; do
echo "cutadapt -q 15,15 -m 36 -o ${file/.tr0/}.trim $file > ${file/.tr0/}.trimlog.txt" >> trimse.sh;
done

#I can't get it to run through launcher so just run it serially, it takes a while to run, so consider breaking up in several jobs
sbatch -o trimse.o%j -e trimse.e%j --mem=200GB trimse.sh

# do we have expected number of *.trim files created?
ls -l *.trim | wc -l

#--How many reads in each sample?
#echo '#!/bin/bash' >sintReads
#echo readCounts.py -f trim -o sintFilt >>sintReads
#sbatch --mem=200GB sintReads

echo '#!/bin/bash' >sintReads
echo readCounts.sh -e trim -o sintFilt >>sintReads
sbatch --mem=200GB sintReads

mkdir ../filteredReads
mv *.trim ../filteredReads

zipper.py -f tr0 -a -9 --launcher -e reckert2017@fau.edu
sbatch zip.slurm


################################################################################
#######################  D E N O V O  R E F E R E N C E  #######################
# Construct denovo reference for aligning reads                                #
#------------------------------------------------------------------------------#
##-- Remove symbiodiniaceae reads

##-- if not already, build a bt reference for the concatenated zoox genomes
#mkdir ~/bin/symGenomes
#cd ~/bin/symGenomes
#echo "bowtie2-build symbConcatGenome.fasta symbConcatGenome" > bowtie2-build
#launcher_creator.py -j bowtie2-build -n bowtie2-build -q shortq7 -t 06:00:00 -e reckert2017@fau.edu
#module load bowtie2-2.3.5.1-gcc-8.3.0-63cvhw5
#sbatch --mem=200GB bowtie2-build.slurm
#module load samtools-1.10-gcc-8.3.0-khgksad
#srun samtools faidx symbConcatGenome.fasta

mkdir symbionts
SYMGENOME=~/bin/symGenomes/symbConcatGenome

2bRAD_bowtie2_launcher.py -g $SYMGENOME -f .trim -n zooxMaps --split -u un -a zoox --aldir symbionts --launcher -e $EMAIL

sbatch --mem=200GB zooxMaps.slurm

ls *trim | cut -d '.' -f 1 >align1
grep "% overall" zooxMaps.e* | cut -d ' ' -f 1 >align2
paste <(awk -F' ' '{print $1}' align1) <(awk -F' ' '{print $1}' align2) >zooxAlignmentRates
rm align1 align2

less zooxAlignmentRates

rm *.sam

zipper.py -f .trim -a -9 --launcher -e $EMAIL
sbatch --mem=200GB zip.slurm


## Denovo reference construction
#-------------------------------------------------------------------------------
#--'uniquing' ('stacking') individual trimmed fastq reads:
ls *.trim.un | perl -pe 's/^(.+)$/uniquerOne.pl $1 >$1\.uni/' > unique

launcher_creator.py -j unique -n unique -q shortq7 -t 06:00:00 -e reckert2017@fau.edu
sbatch --mem=200GB unique.slurm

# Checking there is a .uni for all samples
ls -l *.uni | wc -l

# collecting common tags (= major alleles)
# merging uniqued files (set minInd to >10, or >10% of total number of samples, whichever is greater)

echo 'mergeUniq.pl uni minInd=30 > all.uniq' > allunique

launcher_creator.py -j allunique -n allunique -q shortq7 -t 06:00:00 -e reckert2017@fau.edu
sbatch --mem=200GB allunique.slurm

# discarding tags that have more than 7 observations without reverse-complement
srun awk '!($3>7 && $4==0) && $2!="seq"' all.uniq >all.tab

# creating fasta file out of merged and filtered tags:
srun awk '{print ">"$1"\n"$2}' all.tab >all.fasta

# clustering allowing for up to 3 mismatches (-c 0.91); the most abundant sequence becomes reference
echo '#!/bin/bash' >cdhit
echo cd-hit-est -i all.fasta -o cdh_alltags.fas -aL 1 -aS 1 -g 1 -c 0.91 -M 0 -T 0 >>cdhit
sbatch --mem=200GB -e cdhit.e%j -o cdhit.o%j cdhit

rm *.uni


## Construct Kraken database
#-------------------------------------------------------------------------------
#--We need a Kraken database to search against. If you don't already have one,
#  you need to build one now. otherwise you can skip to running Kraken
#  We can download a compiled standard database:

#mkdir ~/bin/krakenDB
#cd ~/bin/krakenDB
#srun wget https://genome-idx.s3.amazonaws.com/kraken/k2_pluspf_20210127.tar.gz
#echo tar -xvf k2_pluspf_20210127.tar.gz >tar
#launcher_creator.py -j tar -n tar -q mediumq7 -t 24:00:00 -e reckert2017@fau.edu
#sbatch tar.slurm

#--Alternatively, we can build a custom database, which can include
#  the Symbiodiniaceae genomes

#echo '#!/bin/bash' >krakendb.sh
#echo kraken2-build --download-taxonomy --db ~/bin/krakenDB >>>krakendb.sh
#echo kraken2-build --download-library archaea --threads 16 --db ~/bin/krakenDB >>krakendb.sh
#echo kraken2-build --download-library bacteria --threads 16 --db ~/bin/krakenDB >>krakendb.sh
#echo kraken2-build --download-library viral --threads 16 --db ~/bin/krakenDB >>krakendb.sh
#echo kraken2-build --download-library human --threads 16 --db ~/bin/krakenDB >>krakendb.sh
#echo kraken2-build --download-library fungi --threads 16 --db ~/bin/krakenDB >>krakendb.sh
#echo kraken2-build --download-library protozoa --threads 16 --db ~/bin/krakenDB >>krakendb.sh
#echo kraken2-build --download-library UniVec_Core --threads 16 --db ~/bin/krakenDB >>krakendb.sh
#sbatch --mem=200GB -p longq7 -e krakenDB.e%j -o krakenDB.o%j krakendb.sh

#--Format and add Symbiodiniaceae genomes to the database
#cd ~/bin/symGenomes

# Symbiodinium microadriaticum
#sed '/>/ s/$/|kraken:taxid|2951/' Symbiodinium_microadriacticum_genome.scaffold.fasta >S_microadriacticum.fa
# Breviolum minutum
#sed '/>/ s/$/|kraken:taxid|2499525/' Breviolum_minutum.v1.0.genome.fa >B_minutum.fa
# Cladocopium goreaui
#sed '/>/ s/$/|kraken:taxid|2562237/' Cladocopium_goreaui_Genome.Scaffolds.fasta >C_goreaui.fa
# Durusdinium trenchii
#sed '/>/ s/$/|kraken:taxid|1381693/' 102_symbd_genome_scaffold.fa >D_trenchii.fa

#echo '#!/bin/bash' >kdbAdd
#echo kraken2-build --add-to-library ~/bin/symGenomes/S_microadriacticum.fa --db ~/bin/krakenDB >>kdbAdd
#echo kraken2-build --add-to-library ~/bin/symGenomes/B_minutum.fa --db ~/bin/krakenDB >>kdbAdd
#echo kraken2-build --add-to-library ~/bin/symGenomes/C_goreaui.fa --db ~/bin/krakenDB >>kdbAdd
#echo kraken2-build --add-to-library ~/bin/symGenomes/D_trenchii.fa --db ~/bin/krakenDB >>kdbAdd
#sbatch --mem=200GB -o kdbAdd.o%j -e kdbAdd.e%j kdbAdd

#--Finally, build the database
echo '#!/bin/bash' >kdbBuild
echo kraken2-build --build --db ~/bin/krakenDB >>kdbBuild
sbatch --mem=200GB -o kdbBuild.o%j -e kdbBuild.e%j kdbBuild


## Remove potential contamination from reference
#-------------------------------------------------------------------------------
cd ~/2bRAD/sint/fknms/filteredReads

echo '#!/bin/bash' >krakenDB
echo kraken2 --db ~/bin/krakenDB cdh_alltags.fas --threads 16 --classified-out cdh_alltags.contam.fa --unclassified-out cdh_alltags.unclass.fa --report krakenDB.report --output krakenDB.out >>krakenDB

sbatch --mem=200GB -o krakenDB.o%j -e krakenDB.e%j krakenDB

echo '#!/bin/bash' >krakenNT
echo kraken2 --db ~/bin/krakenNT/nt_db cdh_alltags.unclass.fa --threads 16 --classified-out cdh_alltags.class.fa --unclassified-out sint_denovo.fa --report krakenNT.report --output krakenNT.out >>krakenNT

sbatch --mem=200GB -o krakenNT.o%j -e krakenNT.e%j krakenNT

extract_kraken_reads.py -s cdh_alltags.class.fa -k krakenNT.out -r krakenNT.report -t 6073 --include-children -o sint_denovo.fa --append


## Construct denovo genome of 30 pseudo chromosomes from clean major allele tags
#-------------------------------------------------------------------------------
# making fake reference genome (of 30 chromosomes)
mkdir ../mappedReads
mv sint_denovo.fa ../mappedReads
cd ../mappedReads

concatFasta.pl fasta=sint_denovo.fa num=30

# format pseudo genome

GENOME_FASTA=sint_denovo_cc.fasta

echo '#!/bin/bash' >genomeBuild.sh
echo bowtie2-build $GENOME_FASTA $GENOME_FASTA >>genomeBuild.sh
echo samtools faidx $GENOME_FASTA >>genomeBuild.sh

sbatch -o genomeBuild.o%j -e genomeBuild.e%j --mem=200GB genomeBuild.sh


################################################################################
##############  M A P P I N G  R E A D S  T O  R E F E R E N C E  ##############
# Mapping reads to reference and formatting bam files                          #
#------------------------------------------------------------------------------#
# map reads to fake genome:
mv ../filteredReads/*.un .
mv ../filteredReads/symbionts .

GENOME_FASTA=sint_denovo_cc.fasta

# mapping with --local option, enables clipping of mismatching ends (guards against deletions near ends of RAD tags)
2bRAD_bowtie2_launcher.py -f un -g $GENOME_FASTA --launcher -e $EMAIL
sbatch --mem=200GB maps.slurm

ls *un | cut -d '.' -f 1 >align1
grep "% overall" maps.e* | cut -d ' ' -f 1 >align2
>alignmentRates
paste <(awk -F' ' '{print $1}' align1) <(awk -F' ' '{print $1}' align2) >alignmentRates
rm align1 align2

less alignmentRates

ls *.sam | wc -l  # number should match number of trim files

# next stage is compressing, sorting and indexing the SAM files, so they become BAM files:
# bam files will be used for genotyping, population structure, etc.

>s2b
for file in *.sam; do
echo "samtools sort -O bam -o ${file/.sam/}.bam $file && samtools index ${file/.sam/}.bam">>s2b;
done

launcher_creator.py -j s2b -n s2b -q shortq7 -t 06:00:00 -e reckert2017@fau.edu
sbatch --mem=200GB s2b.slurm

ls *bam | wc -l  # should be the same number as number of trim files

zipper.py -a -9 -f sam --launcher -e reckert2017@fau.edu
sbatch zip.slurm

rm *.un


################################################################################
############################  G E N O T Y P I N G  #############################
# "FUZZY genotyping" with ANGSD - without calling actual genotypes but working #
# with genotype likelihoods at each SNP. Optimal for low-coverage data (<10x)  #
#------------------------------------------------------------------------------#
mkdir ../ANGSD
cd ../ANGSD
mv ../mappedReads/*.bam* .

ls *bam >bamsClones

## Assessing base qualities and coverage depth
#-------------------------------------------------------------------------------
# ANGSD settings:
# -minMapQ 20 : only highly unique mappings (prob of erroneous mapping =< 1%)
# -baq 1 : realign around indels (not terribly relevant for 2bRAD reads mapped with --local option)
# -maxDepth : highest total depth (sum over all samples) to assess; set to 10x number of samples
# -minInd : the minimal number of individuals the site must be genotyped in. Reset to 50% of total N at this stage.

export FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 20 -maxDepth 2260 -minInd 113"
export TODO="-doQsDist 1 -doDepth 1 -doCounts 1 -dumpCounts 2"

echo '#!/bin/bash' >sintDD.sh
echo angsd -b bamsClones -GL 1 $FILTERS $TODO -P 1 -out dd >>sintDD.sh

sbatch --mem=200GB -o sintDD.o%j -e sintDD.e%j --mail-user=reckert2017@fau.edu --mail-type=ALL sintDD.sh


# summarizing results (using Misha Matz modified script by Matteo Fumagalli)
echo '#!/bin/bash' >RQC.sh
echo Rscript ~/bin/plotQC.R prefix=dd >>RQC.sh
echo gzip -9 dd.counts >>RQC.sh
sbatch -e RQC.e%j -o RQC.o%j --dependency=afterok:460550 --mem=200GB RQC.sh

# proportion of sites covered at >5x:
cat quality.txt

# scp dd.pdf to laptop to look at distribution of base quality scores, fraction of sites in each sample passing coverage thresholds
# and fraction of sites passing genotyping rates cutoffs. Use these to guide choices of -minQ,  -minIndDepth and -minInd filters in subsequent ANGSD runs

# Identifying clones and technical replicates
#-------------------------------------------------------------------------------
FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 20 -minQ 30 -dosnpstat 1 -doHWE 1 -hwe_pval 1e-5 -sb_pval 1e-5 -hetbias_pval 1e-5 -skipTriallelic 1 -minInd 192 -snp_pval 1e-6 -minMaf 0.05"
TODO="-doMajorMinor 1 -doMaf 1 -doCounts 1 -makeMatrix 1 -doIBS 1 -doCov 1 -doGeno 8 -doBcf 1 -doPost 1 -doGlf 2"

echo '#!/bin/bash' > sintClones.sh
echo angsd -b bamsClones -GL 1 $FILTERS $TODO -P 1 -out sintClones >>sintClones.sh

sbatch --mem=200GB -o sintClones.o%j -e sintClones.e%j -p shortq7 --mail-type=ALL --mail-user=reckert2017@fau.edu sintClones.sh

# use ibs matrix to identify clones with hierachial clustering in R
# scp to local machine and run R markdown chunk


## Removing clones and re-running ANGSD
#-------------------------------------------------------------------------------
mkdir clones
mv sintClones* clones

ls *.bam > bamsNoClones

cat bamsClones | grep -v 'fk_S066.1.trim.un.bt2.bam\|fk_S066.3.trim.un.bt2.bam\|fk_S162.1.trim.un.bt2.bam\|fk_S162.3.trim.un.bt2.bam\|fk_S205.1.trim.un.bt2.bam\|fk_S205.3.trim.un.bt2.bam' >bamsNoClones

FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 20 -minQ 30 -dosnpstat 1 -doHWE 1 -hwe_pval 1e-5 -sb_pval 1e-5 -hetbias_pval 1e-5 -skipTriallelic 1 -minInd 187 -snp_pval 1e-6 -minMaf 0.05"
TODO="-doMajorMinor 1 -doMaf 1 -doCounts 1 -makeMatrix 1 -doIBS 1 -doCov 1 -doGeno 8 -doBcf 1 -doPost 1 -doGlf 2"

echo '#!/bin/bash' > sintNoClones.sh
echo srun angsd -b bamsNoClones -GL 1 $FILTERS $TODO -P 1 -out sintNoClones >> sintNoClones.sh

sbatch --mem=200GB -o sintNoClones.o%j -e sintNoClones.e%j -p shortq7 --mail-type=ALL --mail-user=reckert2017@fau.edu sintNoClones.sh


## How many SNPs?
#-------------------------------------------------------------------------------
grep "filtering:" sintNoClones.e*
# 24,670 SNPs


################################################################################
########################  H E T E R O Z Y G O S I T Y  #########################
# Calculating Heterozygosity across all loci (variant//invariant) using ANGSD  #
# and R script from Misha Matz (xxx)                                           #
#------------------------------------------------------------------------------#
echo '#!/bin/bash' > RHetVar.sh
echo heterozygosity_beagle.R sintNoClones.beagle.gz >> RHetVar.sh

sbatch -e RHet.e%j -o RHet.o%j --mem=200GB --mail-user reckert2017@fau.edu --mail-type=ALL RHetVar.sh


mkdir angsdPopStats

#Note there are no MAF or snp filters so as not to affect allelic frequencies that may change heterozygosity calculations
FILTERS="-uniqueOnly 1 -remove_bads 1  -skipTriallelic 1 -minMapQ 20 -minQ 30 -doHWE 1 -sb_pval 1e-5 -hetbias_pval 1e-5 -minInd 187"

TODO="-doMajorMinor 1 -doMaf 1 -dosnpstat 1 -doPost 2 -doGeno 11 -doGlf 2"

echo '#!/bin/bash' > sintPopStats.sh
echo srun angsd -b bamsNoClones -GL 1 $FILTERS $TODO -P 1 -out sintPopStats >> sintPopStats.sh

sbatch --mem=200GB -o sintPopStats.o%j -e sintPopStats.e%j -p shortq7 --mail-type=ALL --mail-user=reckert2017@fau.edu sintPopStats.sh

mv sintPopStats* angsdPopStats/
cd angsdPopStats


## How many sites?
#-------------------------------------------------------------------------------
grep "filtering:" sintPopStats.e*
# 2,190,950 sites


## Calculate heterozygosity
#-------------------------------------------------------------------------------
echo '#!/bin/bash' > RHet.sh
echo heterozygosity_beagle.R sintPopStats.beagle.gz >> RHet.sh

sbatch -e RHet.e%j -o RHet.o%j --mem=200GB --mail-user reckert2017@fau.edu --mail-type=ALL RHet.sh


################################################################################
###################  P O P U L A T I O N  S T R U C T U R E  ###################
# Calculate population structure from genotype likelihoods using ngsAdmix      #
#------------------------------------------------------------------------------#
# ngsAdmix for K from 2 to 11 : FIRST remove all clones/genotyping replicates!
mkdir ../ngsAdmix
cp *beagle* ../ngsAdmix

zipper.py -f bam -a -9 --launcher -e reckert2017@fau.edu
sbatch zip.slurm

cd ../ngsAdmix

# create a file with 100 replicate simulations for each value of K 1-11 (num pops + 3)
ngsAdmixLauncher.py -f sintNoClones.beagle.gz --maxK 11 -r 50 -n fkSint --launcher -e $EMAIL

sbatch --mem=200GB fkSintNgsAdmix.slurm


## Calculating most likely value of K
#-------------------------------------------------------------------------------
# Next, take the likelihood value from each run of NGSadmix and put them into a file that
# can be used with Clumpak to calculate the most likely K using the methods of Evanno et al. (2005).

>fkSintNgsAdmixLogfile
for log in fkSint*.log; do
grep -Po 'like=\K[^ ]+' $log >> fkSintNgsAdmixLogfile;
done

# Format for CLUMPAK in R
R
# you are now using R in the terminal

logs <- as.data.frame(read.table("fkSintNgsAdmixLogfile"))

#output is organized with 10, 11 preceding 1, 2, 3 etc.
logs$K <- c(rep("10", 50), rep("11", 50), rep("1", 50), rep("2", 50), rep("3", 50),
rep("4", 50), rep("5", 50), rep("6", 50),
    rep("7", 50), rep("8", 50), rep("9", 50))
write.table(logs[, c(2, 1)], "fkSintNgsAdmixLogfile_formatted", row.names = F,
        col.names = F, quote = F)
quit()
# No need to save workspace image [press 'n']
n

# check that your formatted logfile has the appropriate number of entries
cat fkSintNgsAdmixLogfile_formatted | wc -l

# make copies of .qopt files to run structure selector on (.Q files)
for file in fkSint*.qopt; do
filename=$(basename -- "$file" .qopt);
cp "$file" "$filename".Q;
done

mkdir fkSintQ
mv fkSint*Q fkSintQ

zip -r fkSintQ.zip fkSintQ

# scp .zip and formatted logfile to local machine and upload to CLUMPAK (http://clumpak.tau.ac.il/bestK.html)
# and structure selector (https://lmme.ac.cn/StructureSelector/index.html)
# scp sintNoClones* to local machine for further analyses with R


################################################################################
##----------------------------------------------------------------------------##
################################################################################
